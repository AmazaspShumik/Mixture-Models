# -*- coding: utf-8 -*-

import numpy as np
from scipy.stats import multivariate_normal as mvn
from sklearn.cluster import KMeans
import label_binariser as lb
from scipy.misc import logsumexp


    
############################ Mixture Discriminant Analysis ################################    

class MDA(object):
    '''
    Mixture Discriminant Analysis - generative method of classification
    
    A classifier with non-linear decision boundary , generated by fitting class
    conditional densities with mixture of Gaussians. And using using Bayes rule
    to obtain posterior distribution. 
    
    [n = n_samples, m = n_features]
    
    Parameters:
    ------------
    
    Y :  numpy array, shape = [n,1]
           Target Values
          
    X :  numpy array of size 'n x m'
           Expanatory variables
          
    clusters :  list of length 'k' 
           Number of mixture components in each class
          
    init_restarts : int , default = 2
           Number of restarts during parameter initialisation
          
    k : int 
           Number of classes
           
    init_conv_threshold: float, default = 1e-5 
           Convergence threshold for k-means in intialisation step
           
    iter_conv_threshold: float, default = 1e-5
           Convergence threshold for EM algorithm that fits MDA
           
    max_iter_init: int, default = 300
           Maximum number of iterations for k-means on initialisation step
         
    max_iter : int, default = 300
           Maximum number of iterations for EM algorithm
    '''
    
    def __init__(self,Y,X,clusters,k, max_iter_init = 100, init_restarts       = 2, 
                                                           init_conv_theshold  = 1e-5,
                                                           iter_conv_threshold = 1e-10,
                                                           max_iter            = 2,
                                                           verbose             = True):
        
        # preprocess target vector and transform it to ground truth matrix
        self.gt                  =  lb.LabelBinariser(Y,k)                             
        self.Y                   =  self.gt.convert_vec_to_binary_matrix(compress = False)
        self.X                   =  X
        # n - observations; m - dimension  
        self.n, self.m           =  np.shape(X)                
        self.k                   =  k
        # k - number of classes                         
        self.clusters            =  clusters                  
        self.class_prior         =  np.zeros(self.m)
        # mixing probabilities
        self.latent_var_prior    =  [np.ones(clusters[i])/clusters[i] for i in range(self.k)] 
        self.freq                =  np.sum(self.Y, axis = 0)  # number of elements in each class
        # pooled covariance matrix
        self.covar               =  np.eye(self.m)            
        # means
        self.mu                  =  [np.zeros([self.m,clusters[i]]) for i in range(self.k)]
        # responsibilities
        self.responsibilities    =  [np.zeros([self.n,clusters[i]]) for i in range(self.k)]
        # list of lower bounds (expected to be non-increasing series)
        self.lower_bounds        =  []                             
        self.kmeans_maxiter      =  max_iter_init
        self.kmeans_restarts     =  init_restarts
        self.max_iter            =  max_iter
        self.kmeans_theshold     =  init_conv_theshold
        self.mda_threshold       =  iter_conv_threshold
        self.verbose             =  verbose
        
        
    def fit(self):
        '''
        Fit the MDA model to the given training data.
        '''
        self._initialise_params()
        self._iterate()
        
        
    def predict_probs(self, X):
        ''' 
        Calculates posterior probability
        
        Parameters:
        ------------
        
        X: numpy array of size [n,m]
            Explanatory variables
            
        Returns:
        --------
        
        posterior: numpy array of size
             Matrix of posterior probabilities
        '''
        n,m                 = np.shape(X)                   
        assert m == np.shape(self.X)[1], "Number of features is different"        
        posterior           = np.zeros([n,self.k])
        for k,cluster in enumerate(self.clusters):
            class_prob = np.zeros(n)
            for j in range(cluster):
                prob            = mvn.pdf(X,self.mu[k][:,j],self.covar)
                class_prob     += prob*self.latent_var_prior[k][j]
            posterior[:,k]  = class_prob*self.class_prior[k]
        posterior = (posterior.T / np.sum(posterior, axis = 1)).T
        return posterior
        
        
    def predict(self,X):
        '''
        Uses fitted MDA model to predict classes for given explanatory variables
        
        Parameters:
        ------------
        
        X: numpy array of size [unknown,m]
            Explanatory variables
            
        Returns:
        --------
        
        Y_est: numpy array of size [n,m]
            Vector of estimated target values (classes)
        '''
        assert np.shape(X)[1]==np.shape(self.X)[1], "Number of features is different"
        Y_hat     = self.predict_probs(X)
        Y_est     = self.gt.convert_prob_matrix_to_vec(Y_hat)
        return Y_est
        
        
    # ---------------------------  Initialisation + EM iterative algorithm -------------------------------#
        
    def _initialise_params(self):
        '''
        Initialises parameters using k-means to calculate initial responsibilities
        '''
        
        # initialise class priors
        self._class_prior_compute()
        
        # calculate responsibilities using k-means results      
        for i,cluster in enumerate(self.clusters):
            kmeans = KMeans(n_clusters = cluster, 
                            max_iter    = self.kmeans_maxiter,
                            init       = "k-means++",
                            tol        = self.kmeans_theshold)
            kmeans.fit(self.X[self.Y[:,i]==1,:])
            prediction = kmeans.predict(self.X[self.Y[:,i]==1,:])
            for j in range(cluster):
                self.responsibilities[i][self.Y[:,i]==1,j] = 1*(prediction==j)
                
        # initialise parameters of mda through M-step
        self._m_step()
        if self.verbose:
            print "Initialization step complete"
            

    def _iterate(self):
        ''' 
        Iterates between E-step and M-step until change in parameters of model 
        is close to zero
        '''
        for i in range(self.max_iter):
            self._e_step()
            delta                 = self._m_step()
            
            # in optimal point change in parameters should be close to zero
            termination_condition = [dm < self.mda_threshold for dm in delta]
            if False in termination_condition:                
                if self.verbose:
                    print "iteration {0} completed".format(i)
            else:
                if self.verbose:
                    print "algorithm converged"
                break
        
        
    def _e_step(self):
        '''
        Calculates posterior distribution of latent variable for each class
        '''
        log_lvpr    = np.log(self.latent_var_prior)
        for i,resp_k in enumerate(self.responsibilities):
            for j in range(self.clusters[i]):
                log_prior        = mvn.logpdf(self.X,self.mu[i][:,j],self.covar)
                resp_k[:,j]      = log_prior + log_lvpr[i][j]
            normaliser = logsumexp(resp_k, axis = 1)
            self.responsibilities[i]    = np.exp((resp_k.T - normaliser).T)
        
        
    def _m_step(self):
        '''
        M-step of Expectation Maximization Algorithm
        
        Calculates maximum likelihood estimates of class priors, mixing latent variable
        probabilities, means and pooled covariance matrix
        
        Rerturns:
        ---------
        
        delta_mu: numpy array of size 'k x clusters'
            List of l2 chamges in means
        '''
        delta_mu  = []
        covar     = np.zeros([self.m,self.m])
        for i in range(self.k):
            for j in range(self.clusters[i]):
                
                # calculate mixing probabilities
                class_indicator               = self.Y[:,i]*self.responsibilities[i][:,j]
                self.latent_var_prior[i][j]   = np.sum(class_indicator)/self.freq[i]
    
                # calculate means
                weighted_means                = np.sum(self.X.T*class_indicator, axis=1)
                mu                            = weighted_means / np.sum(class_indicator)
                delta_sq                      = np.sum( (self.mu[i][:,j] - mu)**2 )
                self.mu[i][:,j]               = mu
                delta_mu.append(delta_sq)            
                
                # calculate pooled covariance matrix
                centered                      = self.X - np.outer(self.mu[i][:,j],np.ones(self.n)).T
                addition                      = np.dot(centered.T*class_indicator,centered)
                covar                        += addition
                
        self.covar = covar/self.n
        return delta_mu


    def _class_prior_compute(self):
        ''' 
        Computes prior probability of observation being in particular class 
        '''
        self.class_prior = self.freq/np.sum(self.freq)


    
    